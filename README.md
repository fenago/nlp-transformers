# Transformers-for-NLP

## Key Features

Implement models, such as BERT, Reformer, and T5, that outperform classical language models<br>
Compare NLP applications using GPT-3, GPT-2, and other transformers<br>
Analyze advanced use cases, including polysemy, cross-lingual learning, and computer vision. A GitHub BONUS directory with SOA ChatGPT, GPT-3.5-turbo, GPT-4, and DALL-E notebooks. <br>

## Course Description
Transformers are a game-changer for natural language understanding (NLU) and have become one of the pillars of artificial intelligence.<br>

Transformers for Natural Language Processing,  investigates deep learning for machine translations, language modeling, question-answering, and many more NLP domains with transformers.<br>

An Industry 4.0 AI specialist needs to be adaptable; knowing just one NLP platform is not enough anymore. Different platforms have different benefits depending on the application, whether it's cost, flexibility, ease of implementation, results, or performance. In this course, we analyze numerous use cases with Hugging Face, Google Trax, OpenAI, and AllenNLP.<br>

This course takes transformers' capabilities further by combining multiple NLP techniques, such as sentiment analysis, named entity recognition, and semantic role labeling, to analyze complex use cases, such as dissecting fake news on Twitter. Also, see how transformers can create code using just a brief description.<br>

By the end of this NLP course, you will understand transformers from a cognitive science perspective and be proficient in applying pretrained transformer models to various datasets.<br>

## What you will learn
Discover new ways of performing NLP techniques with the latest pretrained transformers<br>
Grasp the workings of the original Transformer, GPT-3, BERT, T5, DeBERTa, and Reformer<br>
Create language understanding Python programs using concepts that outperform classical deep learning models<br>
Apply Python, TensorFlow, and PyTorch programs to sentiment analysis, text summarization, speech recognition, machine translations, and more<br>
Measure the productivity of key transformers to define their scope, potential, and limits in production<br>
## Who This course Is For
If you want to learn about and apply transformers to your natural language (and image) data, this course is for you.<br>

A good understanding of NLP, Python, and deep learning is required to benefit most from this course. Many platforms covered in this course provide interactive user interfaces, which allow readers with a general interest in NLP and AI to follow several labs of this course.<br>

## Running on a cloud platform or in your environment
To run these notebooks on a cloud platform, just click on one of the badges in the table below or run them on your environment.

**Lab 2: Getting Started with the Architecture of the Transformer Model**

Multi_Head_Attention_Sub_Layer: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab02/Multi_Head_Attention_Sub_Layer.ipynb

positional_encoding.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab02/positional_encoding.ipynb

**Lab 3: Fine-Tuning BERT Models**	

BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab03/BERT_Fine_Tuning_Sentence_Classification_GPU.ipynb


**Lab 4 : Pretraining a RoBERTa Model from Scratch**

Pretraining a RoBERTa Model from Scratch
KantaiBERT.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab05/Transformer_tasks.ipynb


**Lab 5: Downstream NLP Tasks with Transformers**		

Transformer_tasks.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab05/Transformer_tasks.ipynb


**Lab 6 Machine Translation with the Transformer**		
		
Trax_translation.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab06/Trax_Translation.ipynb


**Lab 7: The Rise of Suprahuman Transformers with GPT-3 Engines**				
Getting_Started_GPT_3.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab07/Getting_Started_GPT_3.ipynb

Fine_tuning_GPT_3.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab07/Getting_Started_GPT_3.ipynb

**Lab 8 : Applying Transformers to Legal and Financial Documents for AI Text Summarization**				 Summarizing_Text_with_T5.ipynb: 
https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab08/Summarizing_Text_with_T5.ipynb

**Lab 9 : Matching Tokenizers and Datasets**	

Tokenizers.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab09/Tokenizer.ipynb

Training_OpenAI_GPT_2_CH09.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab09/Training_OpenAI_GPT_2_CH09.ipynb

Summarizing_with_ChatGPT.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab09/Summarizing_with_ChatGPT.ipynb

**Lab 10 : Semantic Role Labeling**	

SRL.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab10/SRL.ipynb

Semantic_Role_Labeling_with_ChatGPT.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab10/Semantic_Role_Labeling_with_ChatGPT.ipynb

**Lab 11 : Let Your Data Do the Talking: Story, Questions, and Answers**

QA.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab11/QA.ipynb

Haystack_QA_Pipeline.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab11/Haystack_QA_Pipeline.ipynb

**Lab 12 Detecting Customer Emotions to Make Predictions**

SentimentAnalysis.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab12/SentimentAnalysis.ipynb

**Lab 13 : Analyzing Fake News with Transformers**

Fake_News.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab13/Fake_News.ipynb

Fake_News_Analysis_with_ChatGPT.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab13/Fake_News_Analysis_with_ChatGPT.ipynb

**Lab 14 : Interpreting Black Box Transformer Models**

BertViz.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab14/BertViz.ipynb

Understanding_GPT_2_models_with_Ecco.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab14/Understanding_GPT_2_models_with_Ecco.ipynb

**Lab 15: From NLP to Task-Agnostic Transformer Models**

Vision_Transformers.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab15/Vision_Transformers.ipynb

DALL_E.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab15/DALL_E.ipynb

**Lab 16 : The Emergence of Transformer-Driven Copilots**

Domain_Specific_GPT_3_Functionality.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab16/Domain_Specific_GPT_3_Functionality.ipynb

KantaiBERT_Recommender.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab16/KantaiBERT_Recommender.ipynb

Vision_Transformer_MLP_Mixer.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab16/Vision_Transformer_MLP_Mixer.ipynb

Compact_Convolutional_Transformers.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab16/Compact_Convolutional_Transformers.ipynb

**Lab 17 :üê¨ Consolidation of Suprahuman Transformers with OpenAI ChatGPT and GPT-4**	

Jump_Starting_ChatGPT_with_the_OpenAI_API.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab17/Jump_Starting_ChatGPT_with_the_OpenAI_API.ipynb

ChatGPT_Plus_writes_and_explains_classification.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab17/ChatGPT_Plus_writes_and_explains_classification.ipynb

Getting_Started_OpenAI_GPT_4.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab17/Getting_Started_OpenAI_GPT_4.ipynb

Prompt_Engineering_as_an_alternative_to_fine_tuning.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab17/Prompt_Engineering_as_an_alternative_to_fine_tuning.ipynb

Getting_Started_with_the_DALL_E_2_API.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab17/Getting_Started_with_the_DALL_E_2_API.ipynb

Speaking_with_ChatGPT.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab17/Speaking_with_ChatGPT.ipynb

ALL_in_One.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/Lab17/ALL_in_One.ipynb

**Appendix III: Generic Text Completion with GPT-2**	

OpenAI_GPT_2.ipynb:https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/AppendixIII/OpenAI_GPT_2.ipynb 

**Appendix IV: Custom Text Completion with GPT-2**	

Training_OpenAI_GPT_2.ipynb: https://colab.research.google.com/github/fenago/nlp-transformers/blob/main/AppendixIV/Training_OpenAI_GPT_2.ipynb


# Table of Contents<br>
1.What are Transformers?<br>
2.Getting Started with the Architecture of the Transformer Model<br>
3.Fine-Tuning BERT models<br>
4.Pretraining a RoBERTa Model from Scratch<br>
5.Downstream NLP Tasks with Transformers<br>
6.Machine Translation with the Transformer<br>
7.The Rise of Suprahuman Transformers with GPT-3 Engines<br>
8.Applying Transformers to Legal and Financial Documents for AI Text Summarization<br>
9.Matching Tokenizers and Datasets<br>
10.Semantic Role Labeling with BERT-Based Transformers<br>
11.Let Your Data Do the Talking: Story, Questions, and Answers<br>
12.Detecting Customer Emotions to Make Predictions<br>
13.Analyzing Fake News with Transformers<br>
14.Interpreting Black Box Transformer Models<br>
15.From NLP to Task-Agnostic Transformer Models<br>
16.The Emergence of Transformer-Driven Copilots<br>
17.The Consolidation of Suprahuman Transformers with OpenAI's ChatGPT and GPT-4<br>
Appendix I: Terminology of Transformer Models<br>
Appendix II: Hardware Constraints for Transformer Models<br>
And more!
