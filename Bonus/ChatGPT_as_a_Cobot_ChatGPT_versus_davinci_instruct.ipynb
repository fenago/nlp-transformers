{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DEWyWJz1vL5"
      },
      "source": [
        "#ChatGPT as a Cobot: ChatGPT versus davinci-002\n",
        "**by Denis Rothman and ChatGPT**\n",
        "\n",
        "**September 10,2023 updated to davinci-002** \n",
        "OpenAI model updated\n",
        "\n",
        "In this notebook, I used ChatGPT as a COBOT, a collaborative bot. ChatGPT was my co-worker for this notebook. \n",
        "\n",
        "We(ChatGPT and I) will go through OpenAI davini-002, ChatGPT versus davinci-002,and the length of conversational responses.\n",
        "\n",
        "Let us first explain how ChatGPT can be a cobot.\n",
        "\n",
        "**Denis Rothman**:\n",
        "How can ChatGPT be a Cobot for programmers?\n",
        "\n",
        "**ChatGPT(my co-worker)**:\n",
        "\"As an AI language model, ChatGPT can be a useful tool for programmers in a number of ways. Here are some examples:\n",
        "\n",
        "Code assistance: ChatGPT can be trained on a vast amount of programming language and syntax, which makes it an ideal companion for programmers who need assistance with coding tasks. It can provide suggestions on code structure, syntax, and even point out errors in code, which can save programmers time and improve the quality of their code.\n",
        "\n",
        "Learning: ChatGPT can also be trained on educational materials related to programming, making it a valuable resource for programmers who want to learn new languages, frameworks, or programming concepts. Programmers can ask ChatGPT questions, receive explanations, and even practice coding in a safe and controlled environment.\n",
        "\n",
        "Collaboration: ChatGPT can be used as a collaborative tool for programmers, helping them to communicate and work together more effectively. Programmers can ask ChatGPT to facilitate discussions, share code snippets, and even translate between languages to bridge the gap between different teams or cultures.\n",
        "\n",
        "Overall, ChatGPT can be a valuable Cobot for programmers, helping them to work more efficiently, learn more effectively, and collaborate more seamlessly.\"\n",
        "\n",
        "**Note**:\n",
        "Transformers for NLP,  Lab 7, The Rise of Suprahuman Transformers with GPT-3 Engines describes how to use GPT-3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ-Jti5q2Khy"
      },
      "outputs": [],
      "source": [
        "#Importing openai\n",
        "try:\n",
        "  import openai\n",
        "except:\n",
        "  !pip install openai\n",
        "  import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj7LRArl3ZV6",
        "outputId": "97419767-9460-4743-ac48-1dabd233e55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()\n",
        "\n",
        "#OpenAI Key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oUpdy1DaCht"
      },
      "source": [
        "#1.OpenAI davinci-instruct-beta\n",
        "\n",
        "At this point, I wanted more information on the max_tokens parameters which controls the length of the prompt + the response.\n",
        "\n",
        "I used **ChatGPT** as a **Cobot**, a co-worker:\n",
        "\n",
        "A dialog with ChatGPT on this topic provided the following insights and tips.\n",
        "\n",
        "**Denis**: What is the maximum number of tokens for the davinci-instruct model?\n",
        "\n",
        "**ChatGPT**:As of my knowledge cutoff date of September 2021, the maximum number of tokens for the davinci-instruct model of OpenAI's GPT-3 language model was 2048 tokens. This means that the model can take in a sequence of up to 2048 tokens as input, with each token representing a word, symbol, or punctuation mark.\n",
        "\n",
        "However, it's worth noting that the exact specifications and capabilities of OpenAI's models can change over time, so it's possible that the maximum number of tokens for the davinci-instruct model may have been updated since my knowledge cutoff. It's always best to consult the most up-to-date documentation from OpenAI for the latest information.\n",
        "\n",
        "**Denis**:When I set up davinci-instruct for 2000 tokens, it only replies with a very short answer. How can I force davinci-instruct to provide longer responses?\n",
        "\n",
        "**ChatGPT**: While the davinci-instruct model can accept input sequences of up to 2048 tokens, the length of the response you receive may depend on a variety of factors, such as the complexity of the input prompt, the specific task or prompt you are using, and the level of detail required in the response.\n",
        "\n",
        "If you are finding that the model is only providing very short answers to your prompts, there are a few strategies you can try to encourage it to provide longer responses:\n",
        "\n",
        "Provide more detailed input: The davinci-instruct model works by predicting the most likely next word in a given sequence based on the context and content of the input it has received. If you provide more detailed input that gives the model a clearer idea of what you are looking for, it may be more likely to generate a longer, more informative response.\n",
        "\n",
        "Increase the temperature: The temperature parameter controls the level of randomness and creativity in the model's responses. If you increase the temperature, the model may be more likely to generate longer and more diverse responses. However, be aware that increasing the temperature can also lead to more nonsensical or irrelevant responses.\n",
        "\n",
        "Use a different prompt or task: If you are consistently getting very short answers from the model, it may be helpful to experiment with different prompts or tasks to see if you can get a more robust response. For example, you might try asking the model to generate a longer piece of text, or to answer a series of related questions rather than a single prompt.\n",
        "\n",
        "**It's worth noting that the davinci-instruct model is a highly complex and sophisticated language model**, and there is no guaranteed way to force it to generate longer or more detailed responses. However, by experimenting with different input strategies and parameters, you may be able to find a configuration that works better for your specific needs.\n",
        "\n",
        "**Denis:** thank you\n",
        "**ChatGPT**: You're welcome! If you have any more questions, feel free to ask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6e-Phcq33ZY",
        "outputId": "18e6dce6-05f9-4b01-9260-310a603d7077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The Critique of Pure Reason is a book by Immanuel Kant that is a critique of the limits of human understanding. Kant argues that the human mind is limited in its ability to know the world as it really is.\n"
          ]
        }
      ],
      "source": [
        "#GPT-3 parameters, p2=the prompt as for ChatGPT\n",
        "p1=\"davinci-002\"\n",
        "p2=\"Summarize the Critique of Pure Reason:\"\n",
        "p3=0\n",
        "p4=2000\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE1i6kuOi5So",
        "outputId": "6e5a2eb5-006d-44f4-ff62-b195f5f1df2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "736\n"
          ]
        }
      ],
      "source": [
        "print(len(r[\"text\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biDuS20D4Xwc",
        "outputId": "a7f14cee-2b5d-4674-e285-5199218d3c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The human mind is limited in its ability to know the world as it really is.\n"
          ]
        }
      ],
      "source": [
        "#We continue the dialog\n",
        "p1=\"davinci-002\"\n",
        "p2=\" In the Critique of Pure Reason Kant argues that the human mind is limited in its ability to know the world as it really is. Explain what the limites of the human mind are:\"\n",
        "p3=0\n",
        "p4=120\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5LdZb_d453O"
      },
      "source": [
        "#2.ChatGPT versus davinci-instruct-beta\n",
        "\n",
        "Below is the output of [OpenAI ChatGPT](https://chat.openai.com/auth/login) for the prompt in this notebook.\n",
        "\n",
        "\n",
        "**Denis**: \"Summarize the Critique of Pure Reason by Kant\n",
        "\n",
        "**ChatGPT(excerpt of the response)**:Immanuel Kant's Critique of Pure Reason is a complex and influential philosophical work that aimed to establish the limits of human knowledge and understanding.\n",
        "\n",
        "Kant's central argument in the Critique of Pure Reason is that our understanding of the world is not solely derived from experience but is also shaped by the inherent structure and organization of our minds. He claimed that our minds impose categories and concepts onto the raw data of experience, which allows us to make sense of it but also limits our knowledge to what we can conceptualize.\n",
        "\n",
        ".../...\n",
        "\n",
        "Overall, the Critique of Pure Reason is a complex and influential work that established many of the key ideas of modern philosophy, including the limits of human knowledge, the importance of empirical investigation, and the role of reason in ethical principles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOrAUp_regB1"
      },
      "source": [
        "#3.ChatGPT Cobot on the length of responses\n",
        "\n",
        "davinci-instruct-beta provided short answers, while ChatGPT provided a longer response. It some cases, a conversational chatbot should provide SHORT answers and in other cases,LONG answers depending on the situation. If you're looking for the capital of a country, for example, you'll only need a short answer. If you want an explanation on a topic, you might want a longer answer. This can be controlled with the max_tokens parameter.\n",
        "\n",
        "I asked my ChatGPT cobot co-worker about this:\n",
        "\n",
        "**Denis**:Why are ChatGPT's answers longer than davinci-instruct responses for the same question?\n",
        "\n",
        "**ChatGPT**:There could be several reasons why ChatGPT's answers are longer than responses generated by the davinci-instruct model for the same question.\n",
        "\n",
        "Firstly, it's important to note that ChatGPT and the davinci-instruct model are different models with different architectures, training data, and parameters. While both models are based on the GPT family of language models, they have been trained on different datasets and have different strengths and weaknesses.\n",
        "\n",
        "In general, ChatGPT may be more focused on generating longer and more detailed responses because it has been specifically designed to generate conversational responses to a wide variety of prompts. As a language model trained for conversation, it may be more likely to generate longer and more complex responses in order to simulate a natural conversation and provide a more complete answer.\n",
        "\n",
        "On the other hand, the davinci-instruct model has been designed specifically for generating instructional content and providing step-by-step guidance for various tasks. As such, it may be more focused on generating responses that are concise and to the point, while still providing all the necessary information to complete the task at hand.\n",
        "\n",
        "Additionally, the specific input prompt or question that you provide to each model can also have an impact on the length of the response you receive. If you provide a more detailed or open-ended prompt to ChatGPT, it may generate a longer response in order to provide a more complete answer. On the other hand, if you provide a more focused or task-specific prompt to the davinci-instruct model, it may generate a shorter response that is more directly relevant to the task at hand.\n",
        "\n",
        "Overall, the length and complexity of the responses generated by different language models can depend on a wide variety of factors, including the specific model architecture, training data, parameters, and input prompts.\n",
        "\n",
        "**Denis:**\n",
        "**ChatGPT**:Thanks! You're welcome! Let me know if you have any more questions.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
